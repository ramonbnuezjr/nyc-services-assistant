# Rate Limiting System for NYC Services GPT MVP

## Overview

This document describes the comprehensive rate limiting system implemented for the NYC Services GPT MVP to manage OpenAI API usage within budget constraints.

## Features

### üéØ **MVP-Optimized Design**
- **Default Model**: `gpt-4o-mini` (15x cheaper than GPT-4)
- **Premium Escalation**: Automatic upgrade to `gpt-4` for complex tasks when allowed
- **Token Budget Controls**: Daily and monthly spending limits
- **Development Caching**: Reduces API calls during testing

### üîß **Rate Limiting Components**
- **Leaky Bucket Queue**: Sliding window RPM/TPM tracking
- **Exponential Backoff**: Industry-standard retry with jitter
- **Intelligent Model Selection**: Task-based model routing
- **Usage Monitoring**: Real-time statistics and budget tracking

## Environment Configuration

Create a `.env` file in the project root with these settings:

```bash
# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Rate Limiting - MVP Settings
ALLOW_PREMIUM=false                # Set to true to allow GPT-4 for complex tasks
RPM_MINI=300                       # Requests per minute for gpt-4o-mini
TPM_MINI=180000                    # Tokens per minute for gpt-4o-mini
RPM_41=60                          # Requests per minute for gpt-4
TPM_41=30000                       # Tokens per minute for gpt-4
RPM_EMBED=3000                     # Requests per minute for embeddings
TPM_EMBED=1000000                  # Tokens per minute for embeddings

# Budget Controls (tokens per period)
DAILY_TOKEN_BUDGET=200000          # Daily token limit (adjust for your budget)
MONTHLY_TOKEN_BUDGET=2000000       # Monthly token limit

# Development Settings
DEV_CACHE_TTL_MS=600000           # Cache TTL in milliseconds (10 minutes)
NODE_ENV=development               # Set to 'production' to disable caching

# Retry Configuration
MAX_RETRIES=4                      # Maximum retry attempts
BASE_RETRY_DELAY=0.3               # Base delay for exponential backoff
MAX_RETRY_DELAY=60                 # Maximum delay between retries
```

## Usage Examples

### Basic LLM Usage
```python
from src.models.llm_client import LLMClient

# Initialize with rate limiting
client = LLMClient()

# Generate response with automatic model selection
response = client.generate_response(
    query="How do I apply for unemployment benefits?",
    retrieved_documents=documents,
    max_tokens=200,
    task_hint="simple query",     # Helps with model selection
    allow_premium=False           # Override premium setting
)

print(f"Model used: {response['model']}")
print(f"From cache: {response['from_cache']}")
print(f"Tokens used: {response['tokens_used']}")
```

### Embedding Generation
```python
from src.ingest.data_processor import EmbeddingClient

client = EmbeddingClient()

# Generate embeddings with rate limiting
texts = ["Query 1", "Query 2", "Query 3"]
embeddings = client.get_embeddings(texts)

print(f"Generated {len(embeddings)} embeddings")
```

### Rate Limiting Statistics
```python
from src.models.rate_limiter import rate_limiter

# Get current usage statistics
stats = rate_limiter.get_usage_stats()

for model, model_stats in stats.items():
    if model != "budget":
        print(f"{model}: {model_stats['requests']} ({model_stats['requests_pct']:.1f}%)")
```

## Model Selection Logic

The system automatically chooses the appropriate model based on task complexity:

### **Default: gpt-4o-mini**
- Cost: $0.15/$0.60 per 1K tokens (input/output)
- Use case: Standard NYC service queries
- Performance: Excellent for straightforward questions

### **Premium: gpt-4**
- Cost: $5.00/$15.00 per 1K tokens (input/output)  
- Use case: Complex analysis, legal questions, multi-step problems
- Trigger keywords: "deep", "legal", "multi-step", "strategy", "analysis", "complex"
- Requires: `ALLOW_PREMIUM=true` or `allow_premium=True` parameter

### **Automatic Escalation**
```python
# These queries will use gpt-4 if premium is allowed:
client.generate_response("Analyze complex legal implications...", docs, task_hint="legal analysis")
client.generate_response("Multi-step eligibility determination...", docs, task_hint="complex")

# These will use gpt-4o-mini:
client.generate_response("How do I apply for benefits?", docs, task_hint="simple")
client.generate_response("What documents do I need?", docs)
```

## Cost Optimization Features

### **1. Token Budget Controls**
- **Daily Limit**: Prevents runaway costs
- **Monthly Limit**: Long-term budget management
- **Auto-shutoff**: Stops API calls when limits reached

### **2. Development Caching**
- **Cache Duration**: 10 minutes by default
- **Cache Key**: Based on model + messages + parameters
- **Automatic**: Only active in development mode
- **Savings**: Eliminates repeated API calls during testing

### **3. Efficient Token Management**
- **System Prompt Limits**: Capped at 2000 tokens
- **Response Limits**: Default 300 tokens (vs 500 previously)
- **Accurate Estimation**: Proper token counting for rate limiting

### **4. Graceful Degradation**
- **Mock Fallback**: System continues working when limits hit
- **Quality Maintenance**: Mock responses maintain evaluation flow
- **Error Recovery**: Exponential backoff prevents API hammering

## Rate Limiting Behavior

### **Leaky Bucket Implementation**
- **Sliding Window**: 60-second rolling window for RPM/TPM
- **Proactive Limiting**: Waits for capacity before making requests
- **Real-time Tracking**: Updates usage immediately after API calls

### **Retry Strategy**
- **Exponential Backoff**: Base 0.3s, doubles each attempt
- **Jitter**: ¬±20% randomization to prevent thundering herd
- **Max Attempts**: 4 retries before fallback
- **Max Delay**: 60 seconds cap on retry delays

## Integration with Existing System

The rate limiting system integrates seamlessly with your existing 87% success rate:

### **Baseline Evaluation**
```python
# The evaluation will now use rate limiting automatically
python -m src.tests.baseline_evaluation
```

### **Backwards Compatibility**
- All existing code continues to work
- Optional parameters for fine-tuning
- Graceful fallbacks maintain functionality

### **Performance Impact**
- **Positive**: Prevents rate limit errors
- **Positive**: Reduces API costs significantly  
- **Positive**: Caching speeds up development
- **Minimal**: Slight overhead for tracking (~1ms per call)

## Monitoring & Debugging

### **Usage Statistics**
```python
# View current usage
python test_rate_limiting.py

# Or programmatically:
stats = rate_limiter.get_usage_stats()
```

### **Debug Output**
The system provides detailed logging:
```
üîß Rate limiter initialized - Premium: False, Dev mode: True
üìä API Usage - gpt-4o-mini: 150+45 tokens, $0.0435
‚ö†Ô∏è Rate limit hit, retrying in 1.2s (attempt 1/4)
‚úÖ Retrieved 5 embeddings from cache
```

### **Cost Tracking**
Every API call logs:
- Model used
- Input/output tokens
- Estimated cost
- Cache hit status

## Production Deployment

### **Environment Settings**
```bash
# Production configuration
NODE_ENV=production
ALLOW_PREMIUM=true
DAILY_TOKEN_BUDGET=500000
MONTHLY_TOKEN_BUDGET=5000000
```

### **Monitoring**
- Monitor daily/monthly token usage
- Track model distribution (mini vs premium)
- Watch for rate limit hits
- Review cost per successful query

## Best Practices

### **For MVP Deployment**
1. **Start Conservative**: `ALLOW_PREMIUM=false` initially
2. **Monitor Costs**: Track daily usage closely
3. **Gradual Scaling**: Increase budgets based on performance
4. **Cache Benefits**: Use development mode for testing

### **For Production**
1. **Enable Premium**: For complex queries requiring GPT-4
2. **Set Realistic Budgets**: Based on expected query volume
3. **Monitor Performance**: Balance cost vs success rate
4. **Regular Review**: Adjust limits based on usage patterns

## Troubleshooting

### **Common Issues**

**Rate Limits Hit Immediately**
- Check your OpenAI account limits
- Verify API key has sufficient quota
- Consider increasing budget limits

**All Responses Are Mock**
- Verify `OPENAI_API_KEY` is set correctly
- Check if daily/monthly budget is exceeded
- Ensure API key has sufficient credits

**High Costs**
- Set `ALLOW_PREMIUM=false` to force gpt-4o-mini
- Reduce `max_tokens` in responses
- Lower daily/monthly budgets

**Cache Not Working**
- Ensure `NODE_ENV=development`
- Check if identical requests (model + messages)
- Verify cache TTL settings

This rate limiting system ensures your NYC Services GPT MVP stays within budget while maintaining the 87% success rate you've achieved! üöÄ
